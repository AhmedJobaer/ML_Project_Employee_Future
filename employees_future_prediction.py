# -*- coding: utf-8 -*-
"""Employees future prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-VVaB67o50BBLQP39Dn4QHce33c5e0n
"""



"""# EDA"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use('ggplot') 
pd.set_option('max_columns',200)

df = pd.read_csv("Employee.csv")

df.info()

# top 5 row in the dataset
df.head()

#how many number of columns and  row
df.shape

# check dupliscated data and drop it
num_of_duplicated_data = df.duplicated().sum()
print(num_of_duplicated_data)
df = df.drop_duplicates()

df.shape

#All columns name
df.columns

#All columns name
df.columns

#Check for missing value
df.isna().sum()

import matplotlib.pyplot as plt

# Assume df is your dataframe
for column in df.columns:
    values = df[column].value_counts()
    plt.pie(values, labels=values.index)
    plt.title(column)
    plt.show()

for col in df.columns:
    print(col)
    print(df[col].value_counts())
    print()

fig, axs = plt.subplots(1, 3, figsize=(15, 5))
axs[0].boxplot(df['JoiningYear'])
axs[1].boxplot(df['Age'])
axs[2].boxplot(df['ExperienceInCurrentDomain'])

# label the plots
axs[0].set_title('JoiningYear')
axs[1].set_title('Age')
axs[2].set_title('ExperienceInCurrentDomain')

plt.show()

# df['Gender'] = df['LeaveOrNot'].replace({0:'Leave',1:'Stay'})

plt.figure(figsize=(15,5))
sns.countplot(data = df , x = df['Gender'],hue=df['LeaveOrNot'],palette='vlag')
import pandas as pd


# Group the data by 'Gender' column
grouped = df.groupby(['Gender'])['LeaveOrNot'].value_counts()

# Calculate the percentage of 1 or 0 in the 'LeaveOrNot' column for each group of 'Gender' column
percentage = grouped / grouped.groupby(level=0).sum() * 100
print("0 means Leave and 1 Stay")
print(percentage)

"""Male employee are more likely to leave."""

plt.figure(figsize=(15,5))
sns.countplot(data = df, x = df['PaymentTier'],hue=df['LeaveOrNot'],palette='vlag')

"""In terms of Salary , employee with PaymentTier 3 are most likely to leave"""



plt.figure(figsize=(15,5))
sns.countplot(data = df , x = df['JoiningYear'],hue=df['LeaveOrNot'],palette='vlag')

"""Employees who joined in recent year are most likely to leave the company."""

df.dtypes

df['Education'].unique()

df['EverBenched'].unique()

df['City'].unique()

df['Gender'].unique()

df['Gender'] = df['Gender'].replace({'Male':1,'Female':0})
df['City'] = df['City'].replace({'Bangalore':1, 'Pune':2, 'New Delhi':3})
df['EverBenched'] = df['EverBenched'].replace({'No':0,'Yes':1})
df['Education'] = df['Education'].replace({'Bachelors':1, 'Masters':2, 'PHD':3})

df.describe()

# Check correlation
plt.figure(figsize=(15,9))
sns.heatmap(df.corr(),annot= True,cmap = 'flare')
plt.show()

df.dtypes

# top 5 row in the dataset
df.head()

# Count the unique values for each column
unique_values = df.nunique()

# Print the results
print(unique_values)

# Check correlation
plt.figure(figsize=(14,9))
sns.heatmap(df.corr(),annot= True,cmap = 'flare')
plt.show()

import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt

# load your dataset

X = df.drop('LeaveOrNot', axis=1)
y = df['LeaveOrNot']

# perform univariate feature selection
selector = SelectKBest(f_classif, k=4)
X_new = selector.fit_transform(X, y)

# get the feature scores
scores = -np.log10(selector.pvalues_)

# create a line graph of the feature scores
plt.bar(range(len(X.columns)), scores)
plt.xticks(range(len(X.columns)), X.columns, rotation='vertical')
plt.show()

df = df.drop(['Education', 'City', 'EverBenched', 'ExperienceInCurrentDomain'], axis=1)

unique_counts = df.nunique()
print(unique_counts)

"""# DT"""

import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score



# Split the data into features and labels
X = df.drop('LeaveOrNot', axis=1)
y = df['LeaveOrNot']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(iris.data, iris.target, train_size=0.80)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit the model to the training data
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dt.predict(X_test)

# Print the accuracy score
print("Accuracy: ", accuracy_score(y_test, y_pred))

from sklearn.tree import plot_tree

# Define the Decision Tree model
dt = DecisionTreeClassifier(criterion="entropy", max_depth=8)

# Train the Decision Tree model
dt.fit(X_train, y_train)

# Make predictions on the test set
y_pred_dt = dt.predict(X_test)

# Evaluation using accuracy score
acc_dt = accuracy_score(y_test, y_pred_dt)
print("Decision Tree Accuracy:", acc_dt)

# Evaluation using confusion matrix
confusion_matrix_dt = confusion_matrix(y_test, y_pred_dt)


# Evaluation using classification report
print("Decision Tree classification report:")
print(classification_report(y_test, y_pred_dt))

sns.heatmap(confusion_matrix_dt, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)

#Visualize the tree
plt.figure(figsize=(30,15))
plot_tree(dt, filled=True)
plt.show()

# plot_importance(dt)
# plt.show()

!pip install lime
import lime
import lime.lime_tabular
from __future__ import print_function
np.random.seed(1)

df.columns
X=df[[ 'JoiningYear', 'PaymentTier', 'Age', 'Gender'    ]]

explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=df.columns.values.tolist(), class_names=['Leave company','Stay in Company'], verbose=True, mode='classification')

j = 0
exp = explainer.explain_instance(X.values[j], dt.predict_proba, num_features=15, top_labels=1)
exp



exp.show_in_notebook(show_table=True)

"""# XgBoost"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2)

# Define the XGBoost model
xgb = XGBClassifier()

# Train the model on the training data
xgb.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb.predict(X_test)

# Evaluation using accuracy score
acc = accuracy_score(y_test, y_pred)
print("XGBoost Accuracy:", acc)


# Evaluation using confusion matrix
confusion_matrix_xgb = confusion_matrix(y_test, y_pred)

# Evaluation using classification report
print("XGBoost classification report:")
print(classification_report(y_test, y_pred))

sns.heatmap(confusion_matrix_xgb, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)

from xgboost import plot_tree
import matplotlib.pyplot as plt

# Plot the decision tree
fig, ax = plt.subplots(figsize=(20, 20))
plot_tree(xgb,  num_trees=0, ax=ax)
plt.show()
from xgboost import plot_importance

# Train the model on the training data
xgb.fit(X_train, y_train)

# Plot feature importance
plot_importance(xgb)

explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=df.columns.values.tolist(), class_names=['Leave company','Stay in Company'], verbose=True, mode='classification')

j = 0
exp = explainer.explain_instance(X.values[j], xgb.predict_proba, num_features=15, top_labels=1)
exp

exp.show_in_notebook(show_table=True)

"""# Hybrid"""

from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2)
# Create the decision tree classifier
dt = DecisionTreeClassifier()

# Create the XGBoost classifier
xgb = XGBClassifier()

# Create the ensemble classifier
ensemble = VotingClassifier(estimators=[('dt', dt), ('xgb', xgb)], voting='soft')

# Fit the ensemble classifier on your training data
ensemble.fit(X_train, y_train)

# Use the ensemble classifier to make predictions on your test data
y_pred = ensemble.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
# Compute evaluation metrics
acc1 = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the results
print("Accuracy: {:.2f}".format(acc1))
print("Precision: {:.2f}".format(prec))
print("Recall: {:.2f}".format(rec))
print("F1 Score: {:.2f}".format(f1))

# Print the confusion matrix
conf_matrix_hybrid = confusion_matrix(y_test, y_pred)

sns.heatmap(conf_matrix_hybrid  , square=True, annot=True, cmap='Blues', fmt='d', cbar=False)



from lime import lime_tabular

# create LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=df.columns.values.tolist(), class_names=['Leave company','Stay in Company'], verbose=True, mode='classification')
# get explanation for a single prediction
i = 0
exp = explainer.explain_instance(X_test[i], ensemble.predict_proba, num_features=5)
exp.show_in_notebook(show_all=False)

import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
accuracy=[acc,acc_dt, acc1]
Algorthms=["Gradient Boosting Machine","Decision Tree", "DT-XgBoost"]

#create traces

trace1 = go.Scatter(
    x = Algorthms,
    y= accuracy,
    name='Algortms Name',
    marker =dict(color='rgba(0,255,0,0.5)',
               line =dict(color='rgb(0,0,0)',width=2)),
                text=Algorthms
)
data = [trace1]

layout = go.Layout(barmode = "group",
                  xaxis= dict(title= 'ML Algorithms',ticklen= 8,zeroline= False),
              yaxis= dict(title= 'Prediction Accuracy',ticklen= 8,zeroline= False))
fig = go.Figure(data = data, layout = layout)
iplot(fig)

"""# Deploy"""

#df['Education'] = df['Education'].replace({'Bachelors':1, 'Masters':2, 'PHD':3})
# Get user input
print("Enter the following information:")

JoiningYear = int(input("JoiningYear: "))

PaymentTier = int(input("PaymentTier: "))
Age = int(input("Age: "))

gender_mapping = {'Male': 1, 'Female': 0}
gender_input  = input("Enter Gender: ")
gender_code = lambda x: gender_mapping.get(x, 0)
Gender=(gender_code(gender_input))

# Create a list of the user input
user_input = [ JoiningYear, PaymentTier, Age, Gender,]

prediction_dt = dt.predict([user_input])
prediction_xgb = xgb.predict([user_input])
prediction_hybrid = ensemble.predict([user_input])

# Create a list of the user input
user_input = [ JoiningYear, PaymentTier, Age, Gender]
print("JoiningYear = ", user_input[0], ", PaymentTier = ", user_input[1], ", Age = ", user_input[2], ", Gender = ", user_input[3])

print("\nPrediction By Decision Tree\n")

if prediction_dt == 0:
    print("\tThe Employee  Will Leave The Company\n\n")
else:
    print("The Employee Will Not Leave The Company \n\n")
    

print("\nPrediction By eXtreme Gradient Boosting \n")

if prediction_xgb == 0:
    print("\tThe Employee  Will Leave The Company\n\n")
else:
    print("The Employee Will Not Leave The Company \n\n")


print("\nPrediction By Hybrid (DT & Xgb) \n")

if prediction_hybrid  == 0:
    print("\tThe Employee  Will Leave The Company\n\n")
else:
    print("The Employee Will Not Leave The Company \n\n")